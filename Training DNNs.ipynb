{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "X_valid, X_train = X_train_full[:5000] / 255.0, X_train_full[5000:] / 255.0\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Mnist_Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_5 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_A = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28,28]),\n",
    "    keras.layers.Dense(300, activation=\"relu\"),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")], name=\"Mnist_Model\")\n",
    "\n",
    "model_A.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
    "model_A.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.6976 - accuracy: 0.7692 - val_loss: 0.5008 - val_accuracy: 0.8300\n",
      "Epoch 2/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4864 - accuracy: 0.8299 - val_loss: 0.4466 - val_accuracy: 0.8458\n",
      "Epoch 3/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4415 - accuracy: 0.8462 - val_loss: 0.4239 - val_accuracy: 0.8542\n",
      "Epoch 4/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4135 - accuracy: 0.8539 - val_loss: 0.4121 - val_accuracy: 0.8528\n",
      "Epoch 5/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3948 - accuracy: 0.8611 - val_loss: 0.3861 - val_accuracy: 0.8692\n",
      "Epoch 6/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3784 - accuracy: 0.8658 - val_loss: 0.3806 - val_accuracy: 0.8668\n",
      "Epoch 7/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3653 - accuracy: 0.8709 - val_loss: 0.3720 - val_accuracy: 0.8720\n",
      "Epoch 8/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3548 - accuracy: 0.8734 - val_loss: 0.3602 - val_accuracy: 0.8712\n",
      "Epoch 9/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3438 - accuracy: 0.8766 - val_loss: 0.3667 - val_accuracy: 0.8712\n",
      "Epoch 10/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3360 - accuracy: 0.8804 - val_loss: 0.3493 - val_accuracy: 0.8754\n",
      "Epoch 11/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3260 - accuracy: 0.8848 - val_loss: 0.3465 - val_accuracy: 0.8764\n",
      "Epoch 12/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3193 - accuracy: 0.8853 - val_loss: 0.3537 - val_accuracy: 0.8712\n",
      "Epoch 13/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3130 - accuracy: 0.8881 - val_loss: 0.3299 - val_accuracy: 0.8822\n",
      "Epoch 14/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3045 - accuracy: 0.8907 - val_loss: 0.3504 - val_accuracy: 0.8726\n",
      "Epoch 15/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2980 - accuracy: 0.8926 - val_loss: 0.3533 - val_accuracy: 0.8714\n",
      "Epoch 16/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2924 - accuracy: 0.8953 - val_loss: 0.3459 - val_accuracy: 0.8758\n",
      "Epoch 17/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2864 - accuracy: 0.8969 - val_loss: 0.3224 - val_accuracy: 0.8810\n",
      "Epoch 18/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2819 - accuracy: 0.8984 - val_loss: 0.3273 - val_accuracy: 0.8802\n",
      "Epoch 19/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2760 - accuracy: 0.9010 - val_loss: 0.3077 - val_accuracy: 0.8890\n",
      "Epoch 20/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2699 - accuracy: 0.9027 - val_loss: 0.3125 - val_accuracy: 0.8886\n",
      "Epoch 21/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2666 - accuracy: 0.9035 - val_loss: 0.3334 - val_accuracy: 0.8818\n",
      "Epoch 22/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2606 - accuracy: 0.9059 - val_loss: 0.3146 - val_accuracy: 0.8872\n",
      "Epoch 23/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2562 - accuracy: 0.9074 - val_loss: 0.3218 - val_accuracy: 0.8834\n",
      "Epoch 24/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2526 - accuracy: 0.9094 - val_loss: 0.3130 - val_accuracy: 0.8860\n",
      "Epoch 25/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2475 - accuracy: 0.9111 - val_loss: 0.3069 - val_accuracy: 0.8874\n",
      "Epoch 26/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2433 - accuracy: 0.9132 - val_loss: 0.3196 - val_accuracy: 0.8848\n",
      "Epoch 27/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2391 - accuracy: 0.9146 - val_loss: 0.2987 - val_accuracy: 0.8898\n",
      "Epoch 28/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2347 - accuracy: 0.9163 - val_loss: 0.3078 - val_accuracy: 0.8908\n",
      "Epoch 29/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2316 - accuracy: 0.9174 - val_loss: 0.3049 - val_accuracy: 0.8892\n",
      "Epoch 30/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2284 - accuracy: 0.9177 - val_loss: 0.3074 - val_accuracy: 0.8884\n",
      "Epoch 31/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2238 - accuracy: 0.9193 - val_loss: 0.3044 - val_accuracy: 0.8884\n",
      "Epoch 32/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2207 - accuracy: 0.9206 - val_loss: 0.3128 - val_accuracy: 0.8898\n",
      "Epoch 33/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2166 - accuracy: 0.9224 - val_loss: 0.3027 - val_accuracy: 0.8922\n",
      "Epoch 34/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2133 - accuracy: 0.9235 - val_loss: 0.2934 - val_accuracy: 0.8930\n",
      "Epoch 35/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2099 - accuracy: 0.9240 - val_loss: 0.3004 - val_accuracy: 0.8884\n",
      "Epoch 36/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2068 - accuracy: 0.9253 - val_loss: 0.2889 - val_accuracy: 0.8960\n",
      "Epoch 37/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2034 - accuracy: 0.9266 - val_loss: 0.2882 - val_accuracy: 0.8936\n",
      "Epoch 38/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2005 - accuracy: 0.9286 - val_loss: 0.3235 - val_accuracy: 0.8850\n",
      "Epoch 39/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1967 - accuracy: 0.9300 - val_loss: 0.3011 - val_accuracy: 0.8932\n",
      "Epoch 40/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1942 - accuracy: 0.9304 - val_loss: 0.2951 - val_accuracy: 0.8948\n",
      "Epoch 41/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1919 - accuracy: 0.9305 - val_loss: 0.3114 - val_accuracy: 0.8886\n",
      "Epoch 42/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1885 - accuracy: 0.9325 - val_loss: 0.2926 - val_accuracy: 0.8952\n",
      "Epoch 43/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1849 - accuracy: 0.9351 - val_loss: 0.3094 - val_accuracy: 0.8902\n",
      "Epoch 44/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1835 - accuracy: 0.9348 - val_loss: 0.2938 - val_accuracy: 0.8968\n",
      "Epoch 45/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1788 - accuracy: 0.9363 - val_loss: 0.2936 - val_accuracy: 0.8952\n",
      "Epoch 46/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1759 - accuracy: 0.9381 - val_loss: 0.3181 - val_accuracy: 0.8884\n",
      "Epoch 47/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1742 - accuracy: 0.9388 - val_loss: 0.3037 - val_accuracy: 0.8936\n",
      "Epoch 48/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1707 - accuracy: 0.9391 - val_loss: 0.3048 - val_accuracy: 0.8928\n",
      "Epoch 49/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1694 - accuracy: 0.9392 - val_loss: 0.2938 - val_accuracy: 0.8978\n",
      "Epoch 50/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1663 - accuracy: 0.9408 - val_loss: 0.3035 - val_accuracy: 0.8940\n",
      "Epoch 51/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1636 - accuracy: 0.9418 - val_loss: 0.3318 - val_accuracy: 0.8894\n",
      "Epoch 52/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1609 - accuracy: 0.9428 - val_loss: 0.3041 - val_accuracy: 0.8930\n",
      "Epoch 53/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1588 - accuracy: 0.9438 - val_loss: 0.3193 - val_accuracy: 0.8874\n",
      "Epoch 54/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1562 - accuracy: 0.9451 - val_loss: 0.3230 - val_accuracy: 0.8872\n",
      "Epoch 55/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1524 - accuracy: 0.9465 - val_loss: 0.3294 - val_accuracy: 0.8878\n",
      "Epoch 56/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1519 - accuracy: 0.9467 - val_loss: 0.3142 - val_accuracy: 0.8926\n",
      "Epoch 57/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1495 - accuracy: 0.9466 - val_loss: 0.3030 - val_accuracy: 0.8940\n",
      "Epoch 58/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1464 - accuracy: 0.9477 - val_loss: 0.3145 - val_accuracy: 0.8942\n",
      "Epoch 59/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1440 - accuracy: 0.9487 - val_loss: 0.3180 - val_accuracy: 0.8912\n",
      "Epoch 60/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1419 - accuracy: 0.9500 - val_loss: 0.3072 - val_accuracy: 0.8992\n",
      "Epoch 61/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1405 - accuracy: 0.9496 - val_loss: 0.3095 - val_accuracy: 0.8954\n",
      "Epoch 62/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1366 - accuracy: 0.9519 - val_loss: 0.3072 - val_accuracy: 0.8982\n",
      "Epoch 63/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1356 - accuracy: 0.9523 - val_loss: 0.3058 - val_accuracy: 0.8978\n",
      "Epoch 64/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1341 - accuracy: 0.9529 - val_loss: 0.3090 - val_accuracy: 0.9000\n",
      "Epoch 65/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1312 - accuracy: 0.9537 - val_loss: 0.3202 - val_accuracy: 0.8962\n",
      "Epoch 66/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1295 - accuracy: 0.9546 - val_loss: 0.3550 - val_accuracy: 0.8830\n",
      "Epoch 67/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1270 - accuracy: 0.9556 - val_loss: 0.3611 - val_accuracy: 0.8812\n",
      "Epoch 68/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1255 - accuracy: 0.9575 - val_loss: 0.3138 - val_accuracy: 0.8978\n",
      "Epoch 69/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1226 - accuracy: 0.9575 - val_loss: 0.3297 - val_accuracy: 0.8940\n",
      "Epoch 70/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1202 - accuracy: 0.9573 - val_loss: 0.3203 - val_accuracy: 0.8966\n",
      "Epoch 71/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1195 - accuracy: 0.9581 - val_loss: 0.3273 - val_accuracy: 0.8922\n",
      "Epoch 72/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1164 - accuracy: 0.9591 - val_loss: 0.3192 - val_accuracy: 0.8986\n",
      "Epoch 73/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1152 - accuracy: 0.9598 - val_loss: 0.3200 - val_accuracy: 0.8950\n",
      "Epoch 74/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1135 - accuracy: 0.9603 - val_loss: 0.3288 - val_accuracy: 0.8948\n",
      "Epoch 75/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1117 - accuracy: 0.9607 - val_loss: 0.3307 - val_accuracy: 0.8952\n",
      "Epoch 76/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1114 - accuracy: 0.9617 - val_loss: 0.3122 - val_accuracy: 0.8986\n",
      "Epoch 77/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1073 - accuracy: 0.9633 - val_loss: 0.3410 - val_accuracy: 0.8912\n",
      "Epoch 78/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1059 - accuracy: 0.9633 - val_loss: 0.3386 - val_accuracy: 0.8942\n",
      "Epoch 79/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1030 - accuracy: 0.9653 - val_loss: 0.3397 - val_accuracy: 0.8934\n",
      "Epoch 80/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1036 - accuracy: 0.9639 - val_loss: 0.3241 - val_accuracy: 0.8986\n",
      "Epoch 81/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1012 - accuracy: 0.9649 - val_loss: 0.3265 - val_accuracy: 0.8970\n",
      "Epoch 82/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0983 - accuracy: 0.9665 - val_loss: 0.3396 - val_accuracy: 0.8988\n",
      "Epoch 83/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0966 - accuracy: 0.9664 - val_loss: 0.3488 - val_accuracy: 0.8926\n",
      "Epoch 84/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0973 - accuracy: 0.9675 - val_loss: 0.3616 - val_accuracy: 0.8906\n",
      "Epoch 85/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0945 - accuracy: 0.9677 - val_loss: 0.3504 - val_accuracy: 0.8960\n",
      "Epoch 86/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0928 - accuracy: 0.9681 - val_loss: 0.3828 - val_accuracy: 0.8900\n",
      "Epoch 87/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0913 - accuracy: 0.9685 - val_loss: 0.3427 - val_accuracy: 0.8986\n",
      "Epoch 88/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0891 - accuracy: 0.9696 - val_loss: 0.3389 - val_accuracy: 0.8956\n",
      "Epoch 89/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0883 - accuracy: 0.9695 - val_loss: 0.3504 - val_accuracy: 0.8982\n",
      "Epoch 90/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0851 - accuracy: 0.9712 - val_loss: 0.3444 - val_accuracy: 0.8998\n",
      "Epoch 91/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0839 - accuracy: 0.9721 - val_loss: 0.3401 - val_accuracy: 0.8998\n",
      "Epoch 92/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0831 - accuracy: 0.9722 - val_loss: 0.3601 - val_accuracy: 0.8978\n",
      "Epoch 93/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0823 - accuracy: 0.9722 - val_loss: 0.3428 - val_accuracy: 0.8976\n",
      "Epoch 94/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0804 - accuracy: 0.9736 - val_loss: 0.3661 - val_accuracy: 0.8978\n",
      "Epoch 95/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0795 - accuracy: 0.9737 - val_loss: 0.3455 - val_accuracy: 0.9006\n",
      "Epoch 96/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0786 - accuracy: 0.9733 - val_loss: 0.3599 - val_accuracy: 0.8972\n",
      "Epoch 97/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0752 - accuracy: 0.9754 - val_loss: 0.3794 - val_accuracy: 0.8922\n",
      "Epoch 98/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0748 - accuracy: 0.9749 - val_loss: 0.3556 - val_accuracy: 0.9012\n",
      "Epoch 99/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0749 - accuracy: 0.9750 - val_loss: 0.3767 - val_accuracy: 0.8982\n",
      "Epoch 100/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0724 - accuracy: 0.9756 - val_loss: 0.3771 - val_accuracy: 0.8950\n"
     ]
    }
   ],
   "source": [
    "history = model_A.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Mnist_Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_5 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model_A = keras.models.load_model(\"model_path.h5\") # if we had to load another model\n",
    "# we have to clone model_A , so when we train model_B it will not change also the layers of model_A\n",
    "model_A_clone = keras.models.clone_model(model_A) #clone just the archi not the weights\n",
    "model_A_clone.set_weights(model_A.get_weights())\n",
    "model_A_clone.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_5 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 265,701\n",
      "Trainable params: 265,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_B_on_A = keras.models.Sequential(model_A_clone.layers[:-1])\n",
    "model_B_on_A.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "model_B_on_A.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_5 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 265,701\n",
      "Trainable params: 101\n",
      "Non-trainable params: 265,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# freez the reused layers\n",
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "model_B_on_A.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
    "model_B_on_A.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model for few epochs with freezed layers: this will train the new layers \n",
    "we chould use train and validation sets of the new task B,\n",
    "but here is just an exmple so we use the same data as model A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": " Received a label value of 9 which is outside the valid range of [0, 1).  Label values: 3 6 1 8 5 8 3 7 1 1 1 7 9 0 4 2 7 7 5 3 1 9 7 0 4 2 0 1 9 6 8 7\n\t [[node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits (defined at <ipython-input-30-0728a3e32ba4>:4) ]] [Op:__inference_train_function_449783]\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-0728a3e32ba4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# wa chould use train and validation sets of the new task B,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# but here is just an exmple so we use the same data as model A\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel_B_on_A\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;31m# NOW we can unfreez the resued layers and train the whole NN on the task B\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel_B_on_A\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    838\u001b[0m         \u001b[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    839\u001b[0m         \u001b[1;31m# stateless function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 840\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    841\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    842\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1842\u001b[0m     \"\"\"\n\u001b[1;32m-> 1843\u001b[1;33m     return self._call_flat(\n\u001b[0m\u001b[0;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1923\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m:  Received a label value of 9 which is outside the valid range of [0, 1).  Label values: 3 6 1 8 5 8 3 7 1 1 1 7 9 0 4 2 7 7 5 3 1 9 7 0 4 2 0 1 9 6 8 7\n\t [[node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits (defined at <ipython-input-30-0728a3e32ba4>:4) ]] [Op:__inference_train_function_449783]\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "model_B_on_A.fit(X_train, y_train, epochs=4, validation_data=(X_valid, y_valid))\n",
    "# NOW we can unfreez the resued layers and train the whole NN on the task B\n",
    "for layer in model_B_on_A.layers:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# good practice to use small learning rate to avoid damaging the weights learned from task A    \n",
    "optimizer = keras.optimizers.SGD(lr=1e-4)\n",
    "model_B_on_A.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "model_B_on_A.fit(X_train, y_train, epochs=16, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfert learning is better when used for convolutional neural nets; \n",
    "because these netx tend to learn general feature detectors in lower layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
